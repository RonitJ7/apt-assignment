{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e696986",
   "metadata": {},
   "source": [
    "# 📊 Analysis: Column Mapping Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39822bf",
   "metadata": {},
   "source": [
    "This notebook explores the statistical and structural relationships between the columns to justify the inferred mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11bf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load data and noise reduction\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.describe()\n",
    "\n",
    "df_smoothed = df.copy()\n",
    "\n",
    "columns_to_smooth = df.columns[:5]\n",
    "span = min(1000, len(df) // 200) \n",
    "\n",
    "for column in columns_to_smooth:\n",
    "    if df[column].dtype in ['int64', 'float64']:\n",
    "        df_smoothed[column] = df[column].ewm(span=span, adjust=False).mean()\n",
    "        print(f\"Applied EWMA (span={span}) to column: {column}\")\n",
    "\n",
    "df_smoothed.to_csv('data_smoothed.csv', index=False)\n",
    "print(\"Smoothed data saved to 'data_smoothed.csv'\")\n",
    "\n",
    "# comment the below line to remove noise reduction \n",
    "df = df_smoothed.copy() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e46d01",
   "metadata": {},
   "source": [
    "### 🔍 Step 1: Identifying Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09967e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_counts = {}\n",
    "columns = df.columns\n",
    "for col in columns:\n",
    "    volume_counts[col]=0\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    candidates = []\n",
    "    for col in columns:\n",
    "        value = row[col]\n",
    "        if value > 0 and value == int(value):\n",
    "            candidates.append(col)\n",
    "    if candidates:\n",
    "        max_value = 0\n",
    "        best_col = None\n",
    "        for col in candidates:\n",
    "            if row[col] > max_value:\n",
    "                max_value = row[col]\n",
    "                best_col = col\n",
    "        volume_counts[best_col] += 1\n",
    "\n",
    "for col,count in volume_counts.items():\n",
    "    volume_counts[col] = count / len(df)\n",
    "max_column_volume = max(volume_counts, key=volume_counts.get)\n",
    "max_confidence = volume_counts[max_column_volume]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(volume_counts.keys(), volume_counts.values())\n",
    "plt.title('Probability of Each Column Being Volume')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Thus, Column {max_column_volume} is the most likely candidate for volume with confidence {max_confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262f10",
   "metadata": {},
   "source": [
    "### 🔍 Step 2: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"\\n1. Pearson Correlation Matrix:\")\n",
    "pearson_corr = df.corr(method='pearson')\n",
    "print(pearson_corr.round(5))\n",
    "\n",
    "print(\"\\n2. Spearman Correlation Matrix:\")\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "print(spearman_corr.round(5))\n",
    "\n",
    "plt.figure(figsize=(15, 13))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(pearson_corr, annot=True, cmap='coolwarm', center=0, square=True, fmt='.3f', cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', center=0, square=True, fmt='.3f', cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Spearman Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa340373",
   "metadata": {},
   "source": [
    "### 🔍 Step 3: Row-wise Statistics inference (as applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = [col for col in columns if col != max_column_volume]\n",
    "incomings = {col : 0 for col in filtered_columns}\n",
    "outgoings = {col : 0 for col in filtered_columns}\n",
    "\n",
    "for high_col in filtered_columns:\n",
    "    for low_col in filtered_columns:\n",
    "        if high_col == low_col:\n",
    "            continue\n",
    "            \n",
    "        if (df[high_col] >= df[low_col]).all():\n",
    "            print(f\"{high_col} >= {low_col} for all rows\")\n",
    "            incomings[low_col] += 1\n",
    "            outgoings[high_col] += 1\n",
    "\n",
    "filtered_low = [col for col, value in incomings.items() if value == max(incomings.values())]\n",
    "filtered_high = [col for col, value in outgoings.items() if value == max(outgoings.values())]\n",
    "\n",
    "\n",
    "print(f\"\\nMost incoming column: {filtered_low} with {max(incomings.values())} incoming connections\")\n",
    "print(f\"Most outgoing column: {filtered_high} with {max(outgoings.values()) } outgoing connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = [col for col in columns if col != max_column_volume]\n",
    "\n",
    "df_subset = df[filtered_columns]\n",
    "\n",
    "max_indices = df_subset.values.argmax(axis=1)\n",
    "min_indices = df_subset.values.argmin(axis=1)\n",
    "\n",
    "max_cols = df_subset.columns[max_indices]\n",
    "min_cols = df_subset.columns[min_indices]\n",
    "\n",
    "larger_counts = max_cols.value_counts()\n",
    "smaller_counts = min_cols.value_counts()\n",
    "\n",
    "larger = {col: larger_counts.get(col, 0) for col in filtered_columns}\n",
    "smaller = {col: smaller_counts.get(col, 0) for col in filtered_columns}\n",
    "\n",
    "if 'pulse' in filtered_columns:\n",
    "    pulse_high_mask = max_cols == 'pulse'\n",
    "    pulse_low_mask = min_cols == 'pulse'\n",
    "    \n",
    "    vol_pulse_large = df.loc[pulse_high_mask, max_column_volume].sum()\n",
    "    vol_pulse_small = df.loc[pulse_low_mask, max_column_volume].sum()\n",
    "else:\n",
    "    vol_pulse_large = 0\n",
    "    vol_pulse_small = 0\n",
    "\n",
    "for col in filtered_columns:\n",
    "    print(f\"Column {col} is largest {larger[col]} times and smallest {smaller[col]} times\")\n",
    "\n",
    "total_rows = len(df)\n",
    "larger_prob = {col: count / total_rows for col, count in larger.items()}\n",
    "smaller_prob = {col: count / total_rows for col, count in smaller.items()}\n",
    "\n",
    "# Find columns with highest probabilities\n",
    "max_col_high = max(larger_prob, key=larger_prob.get)\n",
    "max_col_low = max(smaller_prob, key=smaller_prob.get)\n",
    "\n",
    "large_price = larger_prob[max_col_high]\n",
    "small_price = smaller_prob[max_col_low]\n",
    "\n",
    "# Find another column with non-zero probability\n",
    "max_col_price = None\n",
    "for col in filtered_columns:\n",
    "    if ((larger_prob[col] > 0 or smaller_prob[col] > 0) and \n",
    "        col not in (max_col_high, max_col_low)):\n",
    "        max_col_price = col\n",
    "        break\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(larger_prob.keys(), larger_prob.values())\n",
    "plt.title('Probability of Each Column Being High')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(smaller_prob.keys(), smaller_prob.values())\n",
    "plt.title('Probability of Each Column Being Low')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns.tolist()\n",
    "print(\"Columns:\", columns)\n",
    "\n",
    "df_prev = df.shift(1)\n",
    "\n",
    "results = []\n",
    "\n",
    "for open_col in columns:\n",
    "    for close_col in columns:\n",
    "        if open_col == close_col or open_col == max_col_price or close_col == max_col_price:\n",
    "            continue\n",
    "        \n",
    "        diff_sum = abs(df[open_col].iloc[1:] - df_prev[close_col].iloc[1:]).sum()\n",
    "        \n",
    "        results.append((open_col, close_col, diff_sum))\n",
    "        print(f\"{open_col}, {close_col}, {diff_sum}\")\n",
    "\n",
    "min_result = min(results, key=lambda x: x[2])\n",
    "print(f\"\\nLowest difference: {min_result[0]} (open) vs {min_result[1]} (close) = {min_result[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae23e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "feature_columns = columns\n",
    "\n",
    "y = df[max_col_price].values\n",
    "X_full = df.drop(columns=[max_col_price]).values\n",
    "\n",
    "model.fit(X_full, y)\n",
    "\n",
    "y_pred_full = model.predict(X_full)\n",
    "exact_matches = (abs(y - y_pred_full) < 0.01).sum()\n",
    "\n",
    "print(\"Analyzing how dropping each feature affects the model\")\n",
    "\n",
    "base_score = model.score(X_full, y)\n",
    "base_matches = exact_matches\n",
    "\n",
    "print(f\"Baseline model R² score: {base_score:.6f}\")\n",
    "print(f\"Baseline exact matches: {base_matches}/{len(y)}\")\n",
    "\n",
    "for col_to_drop in feature_columns:\n",
    "    if col_to_drop == max_col_price:\n",
    "        print(f\"\\nSkipping target variable: {max_col_price}\")\n",
    "        continue\n",
    "        \n",
    "    X_reduced = df.drop(columns=[max_col_price, col_to_drop]).values\n",
    "    \n",
    "    loo_model = LinearRegression().fit(X_reduced, y)\n",
    "    score = loo_model.score(X_reduced, y)\n",
    "    \n",
    "    loo_y_pred = loo_model.predict(X_reduced)\n",
    "    loo_exact_matches = (abs(y - loo_y_pred) < 0.01).sum()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nDropped feature: {col_to_drop}\")\n",
    "    print(f\"R² score: {score:.6f} (change: {score - base_score:+.6f})\")\n",
    "    print(f\"Exact matches: {loo_exact_matches}/{len(y)} (change: {loo_exact_matches - base_matches:+d})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90791de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_open_close = [col for col in filtered_columns if col not in {max_col_high, max_col_low, max_col_price}]\n",
    "print(f\"Candidate columns: {candidate_open_close}\")\n",
    "\n",
    "candidate_data = df[candidate_open_close].values\n",
    "n_rows = len(df) - 1\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, candidate_open in enumerate(candidate_open_close):\n",
    "    for j, candidate_close in enumerate(candidate_open_close):\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        open_col = candidate_data[:, i]\n",
    "        close_col = candidate_data[:, j]\n",
    "\n",
    "        o1, o2 = open_col[:-1], open_col[1:]\n",
    "        c1, c2 = close_col[:-1], close_col[1:]\n",
    "\n",
    "        condition1 = (o1 > c1) & (o2 > c2) & (o1 > c2)\n",
    "        condition2 = (o1 < c1) & (o2 < c2) & (o1 < c2)\n",
    "\n",
    "        same = np.sum(condition1 | condition2)\n",
    "        same_ratio = same / n_rows\n",
    "\n",
    "        results.append({\n",
    "            'open_col': candidate_open,\n",
    "            'close_col': candidate_close,\n",
    "            'same_absolute': same,\n",
    "            'same_ratio': same_ratio\n",
    "        })\n",
    "        \n",
    "        print(f\"Analyzing open: {candidate_open} and close: {candidate_close}\")\n",
    "        print(f\"Same absolute: {same}\")\n",
    "        print(f\"Same ratio: {same_ratio:.4f}\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fec1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(df)\n",
    "\n",
    "for i, candidate_open in enumerate(candidate_open_close):\n",
    "    for j, candidate_close in enumerate(candidate_open_close):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        works = 0\n",
    "        \n",
    "        open_data = df[candidate_open].values\n",
    "        close_data = df[candidate_close].values\n",
    "        \n",
    "        for k in range(0, size, 1000):\n",
    "            end_idx = min(k + 1000, size)\n",
    "            chunk_end = min(k + 999, size - 1)\n",
    "            \n",
    "            total_diff = close_data[chunk_end] - open_data[k]\n",
    "            \n",
    "            chunk_open = open_data[k:end_idx]\n",
    "            chunk_close = close_data[k:end_idx]\n",
    "            candidate_sum = np.sum(chunk_close - chunk_open)\n",
    "            \n",
    "            if (candidate_sum > 0 and total_diff > 0) or (candidate_sum < 0 and total_diff < 0):\n",
    "                works += 1\n",
    "        \n",
    "        print(f\"Analyzing {candidate_open} -> {candidate_close}: {works}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_open_close)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "print(\"Columns:\", columns)\n",
    "\n",
    "df_prev = df.shift(1)\n",
    "\n",
    "results = []\n",
    "best_open_percentage = float('inf')\n",
    "max_open_col = None\n",
    "max_close_col = None\n",
    "\n",
    "for i, candidate_open in enumerate(candidate_open_close):\n",
    "  for j, candidate_close in enumerate(candidate_open_close):\n",
    "      if i == j:\n",
    "          continue\n",
    "\n",
    "      price_col = max_col_price\n",
    "\n",
    "      closecount = 0\n",
    "      opencount = 0\n",
    "      \n",
    "      open_distance = abs(df[price_col] - df_prev[candidate_open])\n",
    "      close_distance = abs(df[price_col] - df_prev[candidate_close])\n",
    "          \n",
    "      closer_to_open = (open_distance < close_distance).sum()\n",
    "      closer_to_close = (close_distance < open_distance).sum()\n",
    "          \n",
    "      opencount += closer_to_open\n",
    "      closecount += closer_to_close\n",
    "      \n",
    "      open_percentage = 100 * opencount / (opencount + closecount)\n",
    "      \n",
    "      print(f\"Open: {candidate_open}, Close: {candidate_close}, Price: {price_col}\")\n",
    "      print(f\"Closer to Open: {opencount}\")\n",
    "      print(f\"Closer to Close: {closecount}\")\n",
    "      print(f\"Open % (lower the better): {open_percentage:.2f}\")\n",
    "      \n",
    "      results.append({\n",
    "          'open_col': candidate_open,\n",
    "          'close_col': candidate_close,\n",
    "          'open_percentage': open_percentage,\n",
    "          'opencount': opencount,\n",
    "          'closecount': closecount\n",
    "    })\n",
    "      \n",
    "      if open_percentage < best_open_percentage:\n",
    "          best_open_percentage = open_percentage\n",
    "          max_open_col = candidate_open\n",
    "          max_close_col = candidate_close\n",
    "\n",
    "print(f\"\\nOptimal combination:\")\n",
    "print(f\"max_open_col: {max_open_col}\")\n",
    "print(f\"max_close_col: {max_close_col}\")\n",
    "print(f\"Best open percentage: {best_open_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7effbb9",
   "metadata": {},
   "source": [
    "### 📈 Step 4: Candlestick Visual from Mapped Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpdates\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "df['Date_num'] = df.index\n",
    "\n",
    "ohlc_data = df[['Date_num', 'deltaX', 'gamma', 'omega', 'flux']].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "candlestick_ohlc(ax, ohlc_data, width=0.6, colorup='green', colordown='red', alpha=0.8)\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Price')\n",
    "plt.title('Candlestick Chart (deltaX=Open, gamma=High, omega=Low, flux=Close)')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119cc7d",
   "metadata": {},
   "source": [
    "This confirms the structural pattern of price movement and supports the inferred mapping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assgn1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
